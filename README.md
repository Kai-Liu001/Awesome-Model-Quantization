# Awesome-Model-Quantization

This repository contains low-bit quantization papers from 2020 to 2025.

## Table of Contents
- [Awesome-Model-Quantization](#awesome-model-quantization)
  - [Table of Contents](#table-of-contents)
  - [Survey](#survey)
  - [Papers](#papers)
    - [2025](#2025)
    - [2024](#2024)
    - [2023](#2023)
    - [2022](#2022)
    - [2021](#2021)
    - [2020](#2020)

## Survey

- [arXiv 2025](https://arxiv.org/abs/2505.05530) Low-bit Model Quantization for Deep Neural Networks: A Survey

## Papers



### 2025
- [NeurIPS 2025](https://neurips.cc/virtual/2025/poster/119764) QBasicVSR: Temporal Awareness Adaptation Quantization for Video Super-Resolution 
- [NeurIPS 2025](https://arxiv.org/abs/2502.02631) ParetoQ: Improving Scaling Laws in Extremely Low-bit LLM Quantization
- [NeurIPS 2025](https://arxiv.org/abs/2504.09629) Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization 
- [NeurIPS 2025](https://neurips.cc/virtual/2025/poster/118539) DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization 
- [NeurIPS 2025](https://neurips.cc/virtual/2025/poster/115665) Point4Bit: Post Training 4-bit Quantization for Point Cloud 3D Detection 
- [NeurIPS 2025](https://neurips.cc/virtual/2025/poster/119877) Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression 
- [NeurIPS 2025](https://arxiv.org/abs/2505.12266) PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement [[code](https://github.com/xiaoBIGfeng/PMQ-VE)] ![GitHub Repo stars](https://img.shields.io/github/stars/xiaoBIGfeng/PMQ-VE)
- [NeurIPS 2025](https://neurips.cc/virtual/2025/poster/115090) VETA-DiT: Variance-Equalized and Temporally Adaptive Quantization for Efficient 4-bit Diffusion Transformers 
- [NeurIPS 2025](https://arxiv.org/abs/2505.18724) LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning [[code](https://github.com/KingdalfGoodman/LoTA-QAF/blob/main/README.md)] ![GitHub Repo stars](https://img.shields.io/github/stars/KingdalfGoodman/LoTA-QAF)
- [NeurIPS 2025](https://arxiv.org/abs/2506.13771) LittleBit: Ultra Low-Bit Quantization via Latent Factorization 
- [NeurIPS 2025](https://neurips.cc/virtual/2025/poster/120052) HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs 
- [NeurIPS 2025](https://neurips.cc/virtual/2025/poster/117708) Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling 
- [NeurIPS 2025](https://neurips.cc/virtual/2025/poster/119554) Efficient and Generalizable Mixed-Precision Quantization via Topological Entropy 
- [NeurIPS 2025](https://neurips.cc/virtual/2025/poster/119301) QSCA: Quantization with Self-Compensating Auxiliary for Monocular Depth Estimation 

- [ICCV 2025](https://arxiv.org/abs/2404.19248) Scheduling Weight Transitions for Quantization-Aware Training [[code](https://github.com/cvlab-yonsei/TRS)] ![GitHub Repo stars](https://img.shields.io/github/stars/cvlab-yonsei/TRS)
- [ICCV 2025](https://arxiv.org/abs/2507.16782) Task-Specific Zero-shot Quantization-Aware Training for Object Detection [[code](https://github.com/DFQ-Dojo/dfq-toolkit)] ![GitHub Repo stars](https://img.shields.io/github/stars/DFQ-Dojo/dfq-toolkit)
- [ICCV 2025](https://arxiv.org/abs/2503.10959) OuroMamba: A Data-Free Quantization Framework for Vision Mamba
- [ICCV 2025]() Allowing Oscillation Quantization: Overcoming Solution Space Limitation in Low Bit-Width Quantization [[code](https://github.com/muzenc/AOQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/muzenc/AOQ)
- [ICCV 2025](https://arxiv.org/abs/2506.23516) FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization  [[code](https://github.com/Seongyeol-kim/FedWSQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/Seongyeol-kim/FedWSQ)
- [ICCV 2025](https://arxiv.org/abs/2412.16553) Semantic Alignment and Reinforcement for Data-Free Quantization of Vision Transformers  [[code](https://github.com/zysxmu/SARDFQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/zysxmu/SARDFQ)
- [ICCV 2025](https://arxiv.org/abs/2503.06545) QuantCache: Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation [[code](https://github.com/JunyiWuCode/QuantCache)] ![GitHub Repo stars](https://img.shields.io/github/stars/JunyiWuCode/QuantCache)
- [ICCV 2025](https://arxiv.org/abs/2507.19131) MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective 
- [ICCV 2025](https://arxiv.org/abs/2507.12933) DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization [[code](https://github.com/LeeDongYeun/dmq)] ![GitHub Repo stars](https://img.shields.io/github/stars/LeeDongYeun/dmq)
- [ICCV 2025](https://arxiv.org/abs/2503.03088) AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model
- [ICCV 2025](https://arxiv.org/abs/2507.22349) MSQ: Memory-Efficient Bit Sparsification Quantization  
- [ICCV 2025](https://arxiv.org/abs/2402.03666) QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning  [[code](https://github.com/hatchetProject/QuEST)] ![GitHub Repo stars](https://img.shields.io/github/stars/hatchetProject/QuEST)

- [ICML 2025](https://arxiv.org/abs/2505.05799) MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design [[code](https://github.com/cat538/MxMoE)] ![GitHub Repo stars](https://img.shields.io/github/stars/cat538/MxMoE)
- [ICML 2025](https://arxiv.org/abs/2505.04877) Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning 
- [ICML 2025](https://arxiv.org/html/2412.14363v2) ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals [[code](https://github.com/utkarsh-dmx/project-resq)] ![GitHub Repo stars](https://img.shields.io/github/stars/utkarsh-dmx/project-resq)
- [ICML 2025](https://arxiv.org/abs/2405.14917) SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models [[code](https://github.com/Aaronhuang-778/SliM-LLM)] ![GitHub Repo stars](https://img.shields.io/github/stars/Aaronhuang-778/SliM-LLM)
- [ICML 2025](https://arxiv.org/abs/2503.15748v1) PARQ: Piecewise-Affine Regularized Quantization [[code](https://github.com/facebookresearch/parq)] ![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/parq)
- [ICML 2025](https://arxiv.org/abs/2503.22879) Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models [[code](https://github.com/enyac-group/Quamba)] ![GitHub Repo stars](https://img.shields.io/github/stars/enyac-group/Quamba)
- [ICML 2025](https://openreview.net/forum?id=G6DmP9wxeB) LRA-QViT: Integrating Low-Rank Approximation and Quantization for Robust and Efficient Vision Transformers 
- [ICML 2025](https://arxiv.org/abs/2406.13474) BoA: Attention-aware Post-training Quantization without Backpropagation 
- [ICML 2025](https://arxiv.org/abs/2505.03804) MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance [[code](https://github.com/chenzx921020/MoEQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/chenzx921020/MoEQuant)
- [ICML 2025](https://arxiv.org/abs/2502.09720) NestQuant: nested lattice quantization for matrix products and LLMs 
- [ICML 2025](https://arxiv.org/abs/2410.09426) FlatQuant: Flatness Matters for LLM Quantization [[code](https://github.com/ruikangliu/FlatQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/ruikangliu/FlatQuant)
- [ICML 2025](https://arxiv.org/abs/2506.20251) Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models [[code](https://github.com/Thecommonirin/Qresafe)] ![GitHub Repo stars](https://img.shields.io/github/stars/Thecommonirin/Qresafe)
- [ICML 2025](https://arxiv.org/html/2410.09615v1) SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression [[code](https://github.com/Paramathic/slim)] ![GitHub Repo stars](https://img.shields.io/github/stars/Paramathic/slim)
- [ICML 2025](https://arxiv.org/abs/2410.06020) QT-DoG: Quantization-Aware Training for Domain Generalization [[code](https://github.com/saqibjaved1/QT-DoG)] ![GitHub Repo stars](https://img.shields.io/github/stars/saqibjaved1/QT-DoG)
- [ICML 2025](https://arxiv.org/abs/2502.06786) Matryoshka Quantization 
- [ICML 2025](https://arxiv.org/pdf/2505.23651) Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation [[code](https://github.com/ewsn1593/HDRQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/ewsn1593/HDRQ)
- [ICML 2025](https://arxiv.org/abs/2506.22463) Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization [[code](https://github.com/WeizhiGao/MoDiff)] ![GitHub Repo stars](https://img.shields.io/github/stars/WeizhiGao/MoDiff)
- [ICML 2025](https://arxiv.org/abs/2505.14371) Layer-wise Quantization for Quantized Optimistic Dual Averaging 
- [ICML 2025](https://openreview.net/pdf?id=w5fONAEwra) Outlier-Aware Post-Training Quantization for Discrete Graph Diffusion Models 
- [ICML 2025](https://arxiv.org/abs/2501.01144) BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference 
- [ICML 2025](https://arxiv.org/abs/2504.02692) GPTAQ: Efficient Finetuning-Free Quantization with Asymmetric Calibration [[code](https://github.com/Intelligent-Computing-Lab-Panda/GPTAQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/Intelligent-Computing-Lab-Panda/GPTAQ)
- [ICML 2025](https://arxiv.org/abs/2501.17116) Optimizing Large Language Model Training Using FP4 Quantization 
- [ICML 2025](https://arxiv.org/abs/2412.04180) SKIM: Any-bit Quantization Pushing The Limits of Post-Training Quantization 
- [ICML 2025](https://arxiv.org/html/2411.10958v3) SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization [[code](https://github.com/thu-ml/SageAttention)] ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/SageAttention)
- [ICML 2025](https://arxiv.org/abs/2505.22167) Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers [[code](https://github.com/wlfeng0509/Q-VDiT)] ![GitHub Repo stars](https://img.shields.io/github/stars/wlfeng0509/Q-VDiT)
- [ICML 2025](https://arxiv.org/abs/2501.12956) GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models [[code](https://github.com/Evans-Z/GANQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/Evans-Z/GANQ)
- [ICML 2025](https://arxiv.org/abs/2505.07004) GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance [[code](https://github.com/snu-mllab/GuidedQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/snu-mllab/GuidedQuant)

- [AAAI 2025](https://arxiv.org/abs/2412.14628) Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models [[code](https://github.com/Ascend-Research/Qua2SeDiMo)] ![GitHub Repo stars](https://img.shields.io/github/stars/Ascend-Research/Qua2SeDiMo)
- [AAAI 2025](https://arxiv.org/abs/2409.14330) Thinking in Granularity: Dynamic Quantization for Image Super-Resolution by Intriguing Multi-Granularity Clues [[code](https://github.com/MmmingS/Granular-DQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/MmmingS/Granular-DQ)
- [AAAI 2025](https://arxiv.org/pdf/2501.08180) D2-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models [[code](https://github.com/TaylorJocelyn/D2-DPM)] ![GitHub Repo stars](https://img.shields.io/github/stars/TaylorJocelyn/D2-DPM)
- [AAAI 2025](https://arxiv.org/abs/2412.11549) MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion Models [[code](https://github.com/cantbebetter2/MPQ-DM)] ![GitHub Repo stars](https://img.shields.io/github/stars/cantbebetter2/MPQ-DM)

- [CVPR 2025](https://arxiv.org/abs/2411.13918) Quantization without Tears 
- [CVPR 2025](https://arxiv.org/abs/2504.02508) APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformer [[code](https://github.com/GoatWu/APHQ-ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/GoatWu/APHQ-ViT)

- [ICLR 2025](https://arxiv.org/abs/2405.16406) SpinQuant: LLM quantization with learned rotations [[code](https://github.com/facebookresearch/SpinQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/SpinQuant)
- [ICLR 2025](https://arxiv.org/abs/2407.10032) LeanQuant: Accurate and Scalable Large Language Model Quantization with Loss-error-aware Grid 
- [ICLR 2025](https://arxiv.org/abs/2312.07950) CBQ: Cross-Block Quantization for Large Language Models 
- [ICLR 2025](https://arxiv.org/abs/2501.13987) OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting [[code](https://github.com/BrotherHappy/OSTQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/BrotherHappy/OSTQuant)
- [ICLR 2025](https://arxiv.org/pdf/2410.06040v1) QERA: an Analytical Framework for Quantization Error Reconstruction [[code](https://github.com/ChengZhang-98/QERA)] ![GitHub Repo stars](https://img.shields.io/github/stars/ChengZhang-98/QERA)
- [ICLR 2025](https://arxiv.org/abs/2411.05007) Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models [[code](https://github.com/dbw6/svdquant)] ![GitHub Repo stars](https://img.shields.io/github/stars/dbw6/svdquant)
- [ICLR 2025](https://arxiv.org/abs/2501.04304) DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models [[code](https://github.com/ugonfor/DGQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/ugonfor/DGQ)
- [ICLR 2025](https://openreview.net/forum?id=2rnOgyFQgb) SynQ: Accurate Zero-shot Quantization by Synthesis-aware Fine-tuning [[code](https://github.com/snudm-starlab/SynQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/snudm-starlab/SynQ)
- [ICLR 2025](https://arxiv.org/abs/2406.02540) ViDiT-Q: Efficient and accurate quantization of diffusion transformers for image and video generation [[code](https://github.com/thu-nics/ViDiT-Q)] ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/ViDiT-Q)


### 2024
- [AAAI 2024](https://arxiv.org/abs/2311.06798) MetaMix: meta-state precision searcher for mixed-precision activation quantization 
- [AAAI 2024](https://arxiv.org/abs/2309.02784) Norm Tweaking: High-Performance Low-Bit Quantization of Large Language Models [[code](https://github.com/smpanaro/norm-tweaking)] ![GitHub Repo stars](https://img.shields.io/github/stars/smpanaro/norm-tweaking)
- [AAAI 2024](https://arxiv.org/abs/2212.01593) Make RepVGG Greater Again: A Quantization-aware Approach [[code](https://github.com/cxxgtxy/QARepVGG)] ![GitHub Repo stars](https://img.shields.io/github/stars/cxxgtxy/QARepVGG)
- [AAAI 2024](https://arxiv.org/abs/2305.12354) Bi-ViT: Pushing the Limit of Vision Transformer Quantization [[code](https://github.com/YanjingLi0202/Bi-ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/YanjingLi0202/Bi-ViT)
- [AAAI 2024](https://arxiv.org/abs/2312.05693) Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge 
- [AAAI 2024](https://ojs.aaai.org/index.php/AAAI/article/view/29487) AQ-DETR: Low-Bit Quantized Detection Transformer with Auxiliary Queries 
- [AAAI 2024](https://arxiv.org/abs/2306.02272) OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models [[code](https://github.com/xvyaward/owq)] ![GitHub Repo stars](https://img.shields.io/github/stars/xvyaward/owq)
- [AAAI 2024](https://arxiv.org/abs/2401.16760) One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware Quantization Training  
- [AAAI 2024](https://arxiv.org/abs/2303.08302) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [[code](https://github.com/microsoft/DeepSpeed)] ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/DeepSpeed)
- [CVPR 2024](https://arxiv.org/abs/2311.16503) TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models  [[code](https://github.com/ModelTC/TFMQ-DM)] ![GitHub Repo stars](https://img.shields.io/github/stars/ModelTC/TFMQ-DM)
- [CVPR 2024](https://arxiv.org/abs/2401.01543) Retraining-free Model Quantization via One-Shot Weight-Coupling Learning [[code](https://github.com/1hunters/retraining-free-quantization)] ![GitHub Repo stars](https://img.shields.io/github/stars/1hunters/retraining-free-quantization)
- [CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Shang_Enhancing_Post-training_Quantization_Calibration_through_Contrastive_Learning_CVPR_2024_paper.pdf) Enhancing Post-training Quantization Calibration through Contrastive Learning  
- [CVPR 2024](https://arxiv.org/abs/2405.03144) PTQ4SAM: Post-Training Quantization for Segment Anything [[code](https://github.com/chengtao-lv/PTQ4SAM)] ![GitHub Repo stars](https://img.shields.io/github/stars/chengtao-lv/PTQ4SAM)
- [CVPR 2024](https://openreview.net/forum?id=Wxyyc2vvGd) CL-Calib: Enhancing Post-training Quantization Calibration through Contrastive Learning 
- [CVPR 2024](https://arxiv.org/abs/2411.17106) PassionSR: Post-Training Quantization with Adaptive Scale in One-Step Diffusion based Image Super-Resolution [[code](https://github.com/libozhu03/PassionSR)] ![GitHub Repo stars](https://img.shields.io/github/stars/libozhu03/PassionSR)
- [CVPR 2024](https://arxiv.org/html/2311.18129) Mixed-precision quantization for federated learning on resource-constrained heterogeneous devices 
- [CVPR 2024](https://arxiv.org/abs/2404.00928) Instance-Aware Group Quantization for Vision Transformers  
- [CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_Data-Free_Quantization_via_Pseudo-label_Filtering_CVPR_2024_paper.pdf) Data-Free Quantization via Pseudo-label Filtering 
- [CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Ding_Reg-PTQ_Regression-specialized_Post-training_Quantization_for_Fully_Quantized_Object_Detector_CVPR_2024_paper.pdf) Reg-PTQ: Regression-specialized Post-training Quantization for Fully Quantized Object Detector 
- [CVPR 2024](https://arxiv.org/abs/2305.18723) Towards Accurate Post-training Quantization for Diffusion Models [[code](https://github.com/ChangyuanWang17/APQ-DM)] ![GitHub Repo stars](https://img.shields.io/github/stars/ChangyuanWang17/APQ-DM)
- [ECCV 2024](https://arxiv.org/abs/2407.05266) CLAMP-ViT: Contrastive data-free learning for adaptive post-training quantization of vits [[code](https://github.com/georgia-tech-synergy-lab/CLAMP-ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/georgia-tech-synergy-lab/CLAMP-ViT)
- [ECCV 2024](https://arxiv.org/abs/2407.12951) AdaLog: Post-Training Quantization for Vision Transformers with Adaptive Logarithm Quantizer [[code](https://github.com/GoatWu/AdaLog)] ![GitHub Repo stars](https://img.shields.io/github/stars/GoatWu/AdaLog)
- [ECCV 2024](https://arxiv.org/abs/2311.06322) Post-training quantization with progressive calibration and activation relaxing for text-to-image diffusion models [[code](https://github.com/tsa18/PCR)] ![GitHub Repo stars](https://img.shields.io/github/stars/tsa18/PCR)
- [ECCV 2024](https://arxiv.org/html/2401.04339) Memory-Efficient Fine-Tuning for Quantized Diffusion Model [[code](https://github.com/ugonfor/TuneQDM)] ![GitHub Repo stars](https://img.shields.io/github/stars/ugonfor/TuneQDM)
- [ECCV 2024](https://arxiv.org/abs/2405.17873) Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization [[code](https://github.com/thu-nics/MixDQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/MixDQ)
- [ECCV 2024](https://arxiv.org/abs/2407.03917) Timestep-aware correction for quantized diffusion models 
- [ECCV 2024](https://arxiv.org/abs/2407.05266) Clamp-vit: Contrastive data-free learning for adaptive post-training quantization of vits [[code](https://github.com/georgia-tech-synergy-lab/CLAMP-ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/georgia-tech-synergy-lab/CLAMP-ViT)
- [ICLR 2024](https://arxiv.org/abs/2403.12544) AffineQuant: Affine Transformation Quantization for Large Language Models [[code](https://github.com/bytedance/AffineQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/bytedance/AffineQuant)
- [ICLR 2024](https://arxiv.org/abs/2206.09557) LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models [[code](https://github.com/naver-aics/lut-gemm)] ![GitHub Repo stars](https://img.shields.io/github/stars/naver-aics/lut-gemm)
- [ICLR 2024](https://arxiv.org/abs/2310.08659) Loftq: Lora-fine-tuning-aware quantization for large language models [[code](https://github.com/yxli2123/LoftQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/yxli2123/LoftQ)
- [ICLR 2024](https://arxiv.org/abs/2308.13137) OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models [[code](https://github.com/OpenGVLab/OmniQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/OpenGVLab/OmniQuant)
- [ICLR 2024](https://arxiv.org/abs/2310.08041) QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Model [[code](https://github.com/ModelTC/QLLM)] ![GitHub Repo stars](https://img.shields.io/github/stars/ModelTC/QLLM)
- [ICML 2024](https://arxiv.org/abs/2306.07629) SqueezeLLM: Dense-and-Sparse Quantization [[code](https://github.com/SqueezeAILab/SqueezeLLM)] ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM)
- [ICML 2024](https://arxiv.org/abs/2401.10432) A2Q+: Improving Accumulator-Aware Weight Quantization 
- [ICML 2024](https://arxiv.org/abs/2405.03103) Learning from students: Applying t-distributions to explore accurate and efficient formats for llms [[code](https://github.com/cornell-zhang/llm-datatypes)] ![GitHub Repo stars](https://img.shields.io/github/stars/cornell-zhang/llm-datatypes)
- [ICML 2024](https://arxiv.org/abs/2402.02446) LQER:Low-Rank Quantization Error Reconstruction for LLMs [[code](https://github.com/ChengZhang-98/lqer)] ![GitHub Repo stars](https://img.shields.io/github/stars/ChengZhang-98/lqer)
- [ICML 2024](https://arxiv.org/abs/2403.12422) Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization 
- [ICML 2024](https://openreview.net/forum?id=fM9xTkpAdu) Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems 
- [ICML 2024](https://openreview.net/pdf?id=jKUWlgra9b) ERQ: Error Reduction for Post-Training Quantization of Vision Transformers 
- [ICML 2024](https://arxiv.org/abs/2402.02750) KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache [[code](https://github.com/jy-yuan/KIVI)] ![GitHub Repo stars](https://img.shields.io/github/stars/jy-yuan/KIVI)
- [ICML 2024](https://arxiv.org/abs/2402.04291) BiLLM: Pushing the Limit of Post-Training Quantization for LLMs [[code](https://github.com/Aaronhuang-778/BiLLM)] ![GitHub Repo stars](https://img.shields.io/github/stars/Aaronhuang-778/BiLLM)
- [ICML 2024](https://arxiv.org/pdf/2402.04396) QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks [[code](https://github.com/Cornell-RelaxML/quip-sharp)] ![GitHub Repo stars](https://img.shields.io/github/stars/Cornell-RelaxML/quip-sharp)
- [ICML 2024](https://arxiv.org/pdf/2402.04396) Extreme Compression of Large Language Models via Additive Quantization [[code](https://github.com/Vahe1994/AQLM)] ![GitHub Repo stars](https://img.shields.io/github/stars/Vahe1994/AQLM)
- [ICML 2024](https://openreview.net/pdf?id=Uh5XN9d2J4) Outlier-aware Slicing for Post-Training Quantization in Vision Transformer 
- [ICML 2024](https://openreview.net/forum?id=DbyHDYslM7) BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization 
- [ICML 2024](https://openreview.net/pdf?id=fM9xTkpAdu) Reshape and Adapt for Output Quantization ({RAOQ}): Quantization-aware Training for In-memory Computing Systems 
- [NeurIPS 2024](https://arxiv.org/abs/2402.08958) Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers [[code](https://github.com/SamsungLabs/aespa)] ![GitHub Repo stars](https://img.shields.io/github/stars/SamsungLabs/aespa)
- [NeurIPS 2024](https://arxiv.org/abs/2404.00456) QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs [[code](https://github.com/spcl/QuaRot)] ![GitHub Repo stars](https://img.shields.io/github/stars/spcl/QuaRot)
- [NeurIPS 2024](https://arxiv.org/abs/2406.06649) 2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution [[code](https://github.com/Kai-Liu001/2DQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/Kai-Liu001/2DQuant)
- [NeurIPS 2024](https://arxiv.org/abs/2406.00800) MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization [[code](https://github.com/aozhongzhang/magr)] ![GitHub Repo stars](https://img.shields.io/github/stars/aozhongzhang/magr)
- [NeurIPS 2024](https://arxiv.org/abs/2405.18137) Exploiting LLM Quantization 
- [NeurIPS 2024](https://openreview.net/forum?id=HfpV6u0kbX) Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters 
- [NeurIPS 2024](https://arxiv.org/abs/2406.11235) QTIP: Quantization with Trellises and Incoherence Processing [[code](https://github.com/Cornell-RelaxML/qtip)] ![GitHub Repo stars](https://img.shields.io/github/stars/Cornell-RelaxML/qtip)
- [NeurIPS 2024](https://openreview.net/forum?id=dYIqAZXQNV) Generalizing CNNs to graphs with learnable neighborhood quantization [[code](https://github.com/Grosenick-Lab-Cornell/QuantNets)] ![GitHub Repo stars](https://img.shields.io/github/stars/Grosenick-Lab-Cornell/QuantNets)
- [NeurIPS 2024](https://arxiv.org/abs/2410.15526) SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data Parallelism for LLM Training [[code](https://github.com/ByteDance-Seed/SDP4Bit)] ![GitHub Repo stars](https://img.shields.io/github/stars/ByteDance-Seed/SDP4Bit)
- [NeurIPS 2024](https://papers.nips.cc/paper_files/paper/2024/file/ab6a2c6ee757afe43882121281f6065c-Paper-Conference.pdf) Optimal and Approximate Adaptive Stochastic Quantization [[code](https://github.com/ranbenbasat/QUIVER)] ![GitHub Repo stars](https://img.shields.io/github/stars/ranbenbasat/QUIVER)
- [NeurIPS 2024](https://arxiv.org/abs/2406.04333) BitsFusion: 1.99 bits Weight Quantization of Diffusion Model [[code](https://github.com/huggingface/diffusers)] ![GitHub Repo stars](https://img.shields.io/github/stars/huggingface/diffusers)
- [NeurIPS 2024](https://arxiv.org/abs/2412.05926) BiDM: Pushing the Limit of Quantization for Diffusion Models [[code](https://github.com/xingyu-zheng/bidm)] ![GitHub Repo stars](https://img.shields.io/github/stars/xingyu-zheng/bidm)
- [NeurIPS 2024](https://arxiv.org/abs/2406.00800) MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization [[code](https://github.com/AozhongZhang/MagR)] ![GitHub Repo stars](https://img.shields.io/github/stars/AozhongZhang/MagR)
- [NeurIPS 2024](https://arxiv.org/abs/2406.01721) DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs [[code](https://github.com/Hsu1023/DuQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/Hsu1023/DuQuant)
- [NeurIPS 2024](https://arxiv.org/abs/2404.02837) Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models 
- [NeurIPS 2024](https://arxiv.org/abs/2402.08958) Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers 
- [NeurIPS 2024](https://arxiv.org/abs/2412.05926) BiDM: Pushing the Limit of Quantization for Diffusion Models [[code](https://github.com/Xingyu-Zheng/BiDM)] ![GitHub Repo stars](https://img.shields.io/github/stars/Xingyu-Zheng/BiDM)
- [NeurIPS 2024](https://openreview.net/pdf?id=cEtExbAKYV) StepbaQ: Stepping backward as Correction for Quantized Diffusion Models 
- [NeurIPS 2024](https://arxiv.org/abs/2404.02837) Cherry on top: Parameter heterogeneity and quantization in large language models 
- [NeurIPS 2024](https://openreview.net/forum?id=Kw6MRGFx0R) QBB: Quantization with Binary Bases for LLMs 


### 2023
- [AAAI 2023](https://arxiv.org/abs/2109.07865) OMPQ: Orthogonal Mixed Precision Quantization [[code](https://github.com/MAC-AutoML/OMPQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/MAC-AutoML/OMPQ)
- [AAAI 2023](https://arxiv.org/pdf/2302.09572) Rethinking Data-Free Quantization as a Zero-Sum Game [[code](https://github.com/MAC-AutoML/OMPQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/MAC-AutoML/OMPQ)
- [AAAI 2023](https://arxiv.org/abs/2307.10638) Quantized Feature Distillation for Network Quantization 
- [AAAI 2023](https://arxiv.org/abs/2211.16187) Quantization-Aware Interval Bound Propagation for Training Certifiably Robust Quantized Neural Networks [[code](https://github.com/mlech26l/quantization_aware_ibp)] ![GitHub Repo stars](https://img.shields.io/github/stars/mlech26l/quantization_aware_ibp)
- [CVPR 2023](https://arxiv.org/abs/2305.10727) Boost Vision Transformer with GPU-Friendly Sparsity and Quantization 
- [CVPR 2023](https://arxiv.org/abs/2206.00820) NIPQ: Noise proxy-based Integrated Pseudo-Quantization [[code](https://github.com/ECoLab-POSTECH/NIPQ?tab=readme-ov-file)] ![GitHub Repo stars](https://img.shields.io/github/stars/ECoLab-POSTECH/NIPQ?tab=readme-ov-file)
- [CVPR 2023](https://arxiv.org/abs/2212.04780) GENIE: Show Me the Data for Quantization [[code](https://github.com/SamsungLabs/Genie)] ![GitHub Repo stars](https://img.shields.io/github/stars/SamsungLabs/Genie)
- [CVPR 2023](https://arxiv.org/abs/2303.13826) Hard Sample Matters a Lot in Zero-Shot Quantization [[code](https://github.com/lihuantong/HAST)] ![GitHub Repo stars](https://img.shields.io/github/stars/lihuantong/HAST)
- [CVPR 2023](https://openaccess.thecvf.com/content/CVPR2023/papers/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2023_paper.pdf) One-Shot Model for Mixed-Precision Quantization 
- [CVPR 2023](https://arxiv.org/abs/2212.07048) PD-Quant: Post-Training Quantization based on Prediction Difference Metric  [[code](https://github.com/hustvl/PD-Quant)] ![GitHub Repo stars](https://img.shields.io/github/stars/hustvl/PD-Quant)
- [CVPR 2023](https://arxiv.org/abs/2211.15736) Post-training Quantization on Diffusion Models [[code](https://github.com/42Shawn/PTQ4DM)] ![GitHub Repo stars](https://img.shields.io/github/stars/42Shawn/PTQ4DM)
- [CVPR 2023](https://arxiv.org/abs/2303.11906) Solving Oscillation Problem in Post-Training Quantization Through a Theoretical Perspective [[code](https://github.com/bytedance/MRECG?tab=readme-ov-file)] ![GitHub Repo stars](https://img.shields.io/github/stars/bytedance/MRECG?tab=readme-ov-file)
- [CVPR 2023](https://arxiv.org/abs/2211.16056) NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers 
- [CVPR 2023](https://arxiv.org/abs/2303.06869) Adaptive Data-Free Quantization [[code](https://github.com/hfutqian/AdaDFQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/hfutqian/AdaDFQ)
- [CVPR 2023](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Bit-Shrinking_Limiting_Instantaneous_Sharpness_for_Improving_Post-Training_Quantization_CVPR_2023_paper.pdf) Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization 
- [CVPR 2023](https://arxiv.org/abs/2304.00253) Q-DETR: An Efficient Low-Bit Quantized Detection Transformer [[code](https://github.com/SteveTsui/Q-DETR)] ![GitHub Repo stars](https://img.shields.io/github/stars/SteveTsui/Q-DETR)
- [ICCV 2023](https://arxiv.org/abs/2207.01405) I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference [[code](https://github.com/zkkli/I-ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/zkkli/I-ViT)
- [ICCV 2023](https://arxiv.org/abs/2212.08254) RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers [[code](https://github.com/zkkli/RepQ-ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/zkkli/RepQ-ViT)
- [ICCV 2023](https://arxiv.org/abs/2309.13682) Causal-dfq: Causality guided data-free network quantization [[code](https://github.com/42Shawn/Causal-DFQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/42Shawn/Causal-DFQ)
- [ICCV 2023](https://arxiv.org/abs/2302.04304) Q-Diffusion: Quantizing Diffusion Models [[code](https://github.com/Xiuyu-Li/q-diffusion)] ![GitHub Repo stars](https://img.shields.io/github/stars/Xiuyu-Li/q-diffusion)
- [ICCV 2023](https://arxiv.org/abs/2308.07209) Unified data-free compression: Pruning and quantization without fine-tuning 
- [ICLR 2023](https://arxiv.org/abs/2302.00193)  $\rm{A}^{2} Q$: Aggregation-Aware Quantization for Graph Neural Networks 
- [ICLR 2023](https://openreview.net/forum?id=tcbBPnfwxS) GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers [[code](https://github.com/IST-DASLab/gptq)] ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/gptq)
- [ICLR 2023](https://openreview.net/forum?id=s1KljJpAukm) PowerQuant:Automorphism Search For Non-Uniform Quantization 
- [ICLR 2023](https://openreview.net/forum?id=VWm4o4l3V9e) Block and Subword-Scaling Floating-Point (BSFP) : An Efficient Non-Uniform Quantization For Low Precision Inference 
- [ICLR 2023](https://openreview.net/forum?id=7L2mgi0TNEP) A$^{2}$Q:Aggregation-Aware Quantization For Graph Nueral Networks [[code](https://github.com/weihai-98/A-2Q)] ![GitHub Repo stars](https://img.shields.io/github/stars/weihai-98/A-2Q)
- [ICLR 2023](https://arxiv.org/abs/2301.09858) PowerQuant: Automorphism Search for Non-Uniform Quantization 
- [ICML 2023](https://arxiv.org/abs/2211.10438) SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models [[code](https://github.com/mit-han-lab/smoothquant?tab=readme-ov-file)] ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/smoothquant?tab=readme-ov-file)
- [ICML 2023](https://arxiv.org/pdf/2306.00317) FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization [[code](https://github.com/robotseye/FlexRound_LRQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/robotseye/FlexRound_LRQ)
- [ICML 2023](https://arxiv.org/pdf/2302.02210) Oscillation-free Quantization for Low-bit Vision Transformers [[code](https://github.com/nbasyl/OFQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/nbasyl/OFQ)
- [ICML 2023](https://arxiv.org/abs/2301.12017) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases [[code](https://github.com/microsoft/DeepSpeed)] ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/DeepSpeed)
- [ICML 2023](https://arxiv.org/pdf/2302.02390) Quantized Distributed Training of Large Models with Convergence Guarantees 
- [NeurIPS 2023](https://openreview.net/forum?id=3gamyee9Yh) QuantSR: Accurate Low-bit Quantization for Efficient Image Super-Resolution [[code](https://github.com/htqin/QuantSR)] ![GitHub Repo stars](https://img.shields.io/github/stars/htqin/QuantSR)
- [NeurIPS 2023](https://openreview.net/pdf?id=sFGkL5BsPi) Q-DM: An Efficient Low-bit Quantized Diffusion Model 
- [NeurIPS 2023](https://arxiv.org/pdf/2203.14645) REx: Data-Free Residual Quantization Error Expansion 
- [NeurIPS 2023](https://arxiv.org/pdf/2305.10657) PTQD: Accurate Post-Training Quantization for Diffusion Models [[code](https://github.com/ziplab/PTQD)] ![GitHub Repo stars](https://img.shields.io/github/stars/ziplab/PTQD)
- [NeurIPS 2023](https://openreview.net/forum?id=r8LYNleLf9) TexQ: Zero-shot Network Quantization with Texture Feature Distribution Calibration
- [NeurIPS 2023](https://openreview.net/pdf?id=N56hAiQvot) PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile [[code](https://github.com/PeiyanFlying/PackQViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/PeiyanFlying/PackQViT)
- [NeurIPS 2023](https://arxiv.org/abs/2305.19268) Intriguing Properties of Quantization at Scale 
- [NeurIPS 2023](https://arxiv.org/abs/2305.14314) QLORA: Efficient Finetuning of Quantized LLMs  [[code](https://github.com/artidoro/qlora)] ![GitHub Repo stars](https://img.shields.io/github/stars/artidoro/qlora)
- [NeurIPS 2023](https://arxiv.org/abs/2306.11987) Training Transformers with 4-bit Integers [[code](https://github.com/xijiu9/Train_Transformers_with_INT4)] ![GitHub Repo stars](https://img.shields.io/github/stars/xijiu9/Train_Transformers_with_INT4)
- [NeurIPS 2023](https://arxiv.org/abs/2307.13304) QuIP: 2-Bit Quantization of Large Language Models With Guarantees [[code](https://github.com/Cornell-RelaxML/QuIP)] ![GitHub Repo stars](https://img.shields.io/github/stars/Cornell-RelaxML/QuIP)
- [NeurIPS 2023](https://papers.nips.cc/paper_files/paper/2023/file/7183f4fc87598f6c6e947b96714acbd6-Paper-Conference.pdf) Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization 
- [NeurIPS 2023](https://arxiv.org/abs/2402.17710) Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers 
- [NeurIPS 2023](https://papers.nips.cc/paper_files/paper/2023/file/400a2e6a82520b690810b97fd67fcc4e-Paper-Conference.pdf) Towards Efficient and Accurate Winograd Convolution via Full Quantization 
- [NeurIPS 2023](https://papers.nips.cc/paper_files/paper/2023/file/983591c3e9a0dc94a99134b3238bbe52-Paper-Conference.pdf) Temporal Dynamic Quantization for Diffusion Models [[code](https://github.com/ECoLab-POSTECH/TDQ_NeurIPS2023)] ![GitHub Repo stars](https://img.shields.io/github/stars/ECoLab-POSTECH/TDQ_NeurIPS2023)
- [NeurIPS 2023](https://papers.nips.cc/paper_files/paper/2023/file/c48bc80aa5d3cbbdd712d1cc107b8319-Paper-Conference.pdf) Pruning vs Quantization: Which is Better? [[code](https://github.com/Qualcomm-AI-research/pruning-vs-quantization)] ![GitHub Repo stars](https://img.shields.io/github/stars/Qualcomm-AI-research/pruning-vs-quantization)


### 2022
- [CVPR 2022](https://arxiv.org/abs/2111.14826) Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation [[code](https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization)] ![GitHub Repo stars](https://img.shields.io/github/stars/liuzechun/Nonuniform-to-Uniform-Quantization)
- [CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Learnable_Lookup_Table_for_Neural_Network_Quantization_CVPR_2022_paper.pdf) Learnable Lookup Table for Neural Network Quantization  [[code](https://github.com/The-Learning-And-Vision-Atelier-LAVA/LLT)] ![GitHub Repo stars](https://img.shields.io/github/stars/The-Learning-And-Vision-Atelier-LAVA/LLT)
- [CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Jeon_Mr.BiQ_Post-Training_Non-Uniform_Quantization_Based_on_Minimizing_the_Reconstruction_Error_CVPR_2022_paper.pdf) Mr.BiQ: Post-Training Non-Uniform Quantization based on Minimizing the Reconstruction Error  
- [CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Chikin_Data-Free_Network_Compression_via_Parametric_Non-Uniform_Mixed_Precision_Quantization_CVPR_2022_paper.pdf) Data-Free Network Compression via Parametric Non-uniform Mixed Precision Quantization 
- [CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Instance-Aware_Dynamic_Neural_Network_Quantization_CVPR_2022_paper.pdf) Instance-Aware Dynamic Neural Network Quantization [[code](https://github.com/mehmetemreakbulut/instance-aware-dynamic-quantization)] ![GitHub Repo stars](https://img.shields.io/github/stars/mehmetemreakbulut/instance-aware-dynamic-quantization)
- [CVPR 2022](https://arxiv.org/abs/2111.09136) IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization [[code](https://github.com/zysxmu/IntraQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/zysxmu/IntraQ)
- [ECCV 2022](https://arxiv.org/abs/2207.10345) CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution 
- [ECCV 2022](https://arxiv.org/pdf/2111.12293) PTQ4ViT: Post-training Quantization for Vision Transformers with Twin Uniform Quantization [[code](https://github.com/hahnyuan/PTQ4ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/hahnyuan/PTQ4ViT)
- [ECCV 2022](https://arxiv.org/pdf/2207.10188) Bitwidth-Adaptive Quantization-Aware Neural Network Training: A Meta-Learning Approach [[code](https://github.com/jsjs0369/MEBQAT)] ![GitHub Repo stars](https://img.shields.io/github/stars/jsjs0369/MEBQAT)
- [ECCV 2022](https://arxiv.org/pdf/2203.08368) Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance [[code](https://github.com/1hunters/LIMPQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/1hunters/LIMPQ)
- [ECCV 2022](https://arxiv.org/pdf/2203.02250) Patch Similarity Aware Data-Free Quantization for Vision Transformers [[code](https://github.com/zkkli/PSAQ-ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/zkkli/PSAQ-ViT)
- [ECCV 2022](https://arxiv.org/pdf/2007.07743) Fine-grained Data Distribution Alignment for Post-Training Quantization [[code](https://github.com/zysxmu/FDDA)] ![GitHub Repo stars](https://img.shields.io/github/stars/zysxmu/FDDA)
- [ECCV 2022](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710657.pdf) Non-Uniform Step Size Quantization for Accurate Post-Training Quantization [[code](https://github.com/sogh5/SubsetQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/sogh5/SubsetQ)
- [ICLR 2022](https://arxiv.org/pdf/2110.02861) 8-bit Optimizers via Block-wise Quantization 
- [ICLR 2022](https://arxiv.org/abs/2202.05239) F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization [[code](https://github.com/snap-research/F8Net)] ![GitHub Repo stars](https://img.shields.io/github/stars/snap-research/F8Net)
- [ICLR 2022](https://arxiv.org/abs/2202.07471) SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation [[code](https://github.com/clevercool/SQuant)] ![GitHub Repo stars](https://img.shields.io/github/stars/clevercool/SQuant)
- [ICLR 2022](https://arxiv.org/abs/2203.05740) QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization [[code](https://github.com/wimh966/QDrop )] ![GitHub Repo stars](https://img.shields.io/github/stars/wimh966/QDrop )
- [ICLR 2022](https://openreview.net/forum?id=3HJOA-1hb0e) Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization 
- [ICLR 2022](https://arxiv.org/abs/2110.02456) VC dimension of partially quantized neural networks in the overparametrized regime. [[code](https://github.com/YutongWangUMich/HANN)] ![GitHub Repo stars](https://img.shields.io/github/stars/YutongWangUMich/HANN)
- [ICML 2022](https://arxiv.org/abs/2206.06501) Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training 
- [ICML 2022](https://arxiv.org/pdf/2203.11086) Overcoming Oscillations in Quantization-Aware Training [[code](https://github.com/qualcomm-ai-research/oscillations-qat)] ![GitHub Repo stars](https://img.shields.io/github/stars/qualcomm-ai-research/oscillations-qat)
- [ICML 2022](https://arxiv.org/pdf/2206.06501) Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training 
- [ICML 2022](https://arxiv.org/pdf/2206.04459) SDQ:Stochastic Differentiable Quantization with Mixed Precision 
- [IJCAI 2022](https://arxiv.org/abs/2204.12322) RAPQ: Rescuing Accuracy for Power-of-Two Low-bit Post-training Quantization [[code](https://github.com/BillAmihom/RAPQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/BillAmihom/RAPQ)
- [IJCAI 2022](https://www.ijcai.org/proceedings/2022/0504.pdf) MultiQuant: Training Once for Multi-bit Quantization of Neural Networks 
- [IJCAI 2022](https://arxiv.org/abs/2111.13824) FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer [[code](https://github.com/megvii-research/FQ-ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/megvii-research/FQ-ViT)
- [NeurIPS 2022](https://papers.nips.cc/paper_files/paper/2022/file/86e7ebb16d33d59e62d1b0a079ea058d-Paper-Conference.pdf) Entropy-driven mixed-precision quantization for deep network design [[code](https://github.com/alibaba/lightweight-neural-architecture-search)] ![GitHub Repo stars](https://img.shields.io/github/stars/alibaba/lightweight-neural-architecture-search)
- [NeurIPS 2022](https://arxiv.org/abs/2109.15082) Towards Efficient Post-training Quantization of Pre-trained Language Models 
- [NeurIPS 2022](https://arxiv.org/abs/2210.06707) Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer [[code](https://github.com/YanjingLi0202/Q-ViT)] ![GitHub Repo stars](https://img.shields.io/github/stars/YanjingLi0202/Q-ViT)
- [NeurIPS 2022](https://openreview.net/pdf?id=F7NQzsl334D) ClimbQ: Class Imbalanced Quantization Enabling Robustness on Efficient Inferences [[code](https://github.com/tinganchen/ClimbQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/tinganchen/ClimbQ)
- [NeurIPS 2022](https://arxiv.org/abs/2212.10200) Redistribution of Weights and Activations for AdderNet Quantization 
- [NeurIPS 2022](https://openreview.net/pdf?id=L7n7BPTVAr3) Leveraging inter-layer dependency for post-training quantization 
- [NeurIPS 2022](https://arxiv.org/abs/2206.01861) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers 
- [NeurIPS 2022](https://openreview.net/forum?id=L7n7BPTVAr3) Leveraging Inter-Layer Dependency for Post -Training Quantization 


### 2021
- [AAAI 2021](https://ojs.aaai.org/index.php/AAAI/article/view/16474/16281) Optimizing information theory based bitwise bottlenecks for efficient mixed-precision activation quantization 
- [AAAI 2021](https://arxiv.org/abs/2102.04782) Distribution Adaptive INT8 Quantization for Training CNNs  [[code](https://github.com/alibaba/Deep-Vision?tab=readme-ov-file)] ![GitHub Repo stars](https://img.shields.io/github/stars/alibaba/Deep-Vision?tab=readme-ov-file)
- [AAAI 2021](https://arxiv.org/abs/2009.14502) Stochastic Precision Ensemble: Self‐Knowledge Distillation for Quantized Deep Neural Networks 
- [AAAI 2021](https://arxiv.org/abs/2007.02017) FracBits: Mixed Precision Quantization via Fractional Bit-Widths [[code](https://github.com/deJQK/FracBits)] ![GitHub Repo stars](https://img.shields.io/github/stars/deJQK/FracBits)
- [AAAI 2021](https://arxiv.org/abs/2002.09049) Post-training Quantization with Multiple Points: Mixed Precision without Mixed Precision 
- [AAAI 2021](https://cdn.aaai.org/ojs/16263/16263-13-19757-1-2-20210518.pdf) Training Binary Neural Network without Batch Normalization for Image Super-Resolution 
- [AAAI 2021](https://ojs.aaai.org/index.php/AAAI/article/view/16306) SA-BNN: State-­Aware Binary Neural Network 
- [CVPR 2021](https://arxiv.org/abs/2010.15703) Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks. [[code](https://github.com/uber-research/permute-quantize-finetune)] ![GitHub Repo stars](https://img.shields.io/github/stars/uber-research/permute-quantize-finetune)
- [CVPR 2021](https://arxiv.org/abs/2103.01049) Diversifying Sample Generation for Accurate Data-Free Quantization 
- [CVPR 2021](https://arxiv.org/abs/2103.07156) Learnable Companding Quantization for Accurate Low-bit Neural Networks  
- [CVPR 2021](https://arxiv.org/abs/2103.15263) Zero-shot Adversarial Quantization 
- [CVPR 2021](https://arxiv.org/abs/2104.00903) Network Quantization with Element-wise Gradient Scaling [[code](https://github.com/cvlab-yonsei/EWGS)] ![GitHub Repo stars](https://img.shields.io/github/stars/cvlab-yonsei/EWGS)
- [CVPR 2021](https://openaccess.thecvf.com/content/CVPR2021/papers/Oh_Automated_Log-Scale_Quantization_for_Low-Cost_Deep_Neural_Networks_CVPR_2021_paper.pdf) Automated Log-Scale Quantization for Low-Cost Deep Neural Networks 
- [CVPR 2021](https://openaccess.thecvf.com/content/CVPR2021/papers/Kryzhanovskiy_QPP_Real-Time_Quantization_Parameter_Prediction_for_Deep_Neural_Networks_CVPR_2021_paper.pdf) QPP: Real-Time Quantization Parameter Prediction for Deep Neural Networks 
- [ICLR 2021](https://arxiv.org/abs/2006.10518) Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming [[code](https://github.com/itayhubara/CalibTIP)] ![GitHub Repo stars](https://img.shields.io/github/stars/itayhubara/CalibTIP)
- [ICLR 2021](https://arxiv.org/abs/2102.05426) BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction [[code](https://github.com/yhhhli/BRECQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/yhhhli/BRECQ)
- [ICLR 2021](https://arxiv.org/abs/2006.08173) Neural gradients are near-lognormal: improved quantized and sparse training 
- [ICLR 2021](https://arxiv.org/abs/2004.07320) Training with Quantization Noise for Extreme Model Compression 
- [ICLR 2021](https://openreview.net/forum?id=3SV-ZePhnZM) Incremental few-shot learning via vector quantization in deep embedded space 
- [ICLR 2021](https://arxiv.org/abs/2008.05000) Degree-Quant: Quantization-Aware Training for Graph Neural Networks [[code](https://github.com/camlsys/degree-quant)] ![GitHub Repo stars](https://img.shields.io/github/stars/camlsys/degree-quant)
- [ICLR 2021](https://arxiv.org/abs/2102.10462) BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization 
- [ICLR 2021](https://openreview.net/forum?id=Qr0aRliE_Hb) Simple Augmentation Goes a Long Way: ADRL for DNN Quantization 
- [ICML 2021](https://arxiv.org/abs/2106.02295) Differentiable dynamic quantization with mixed precision and adaptive resolution 
- [ICML 2021](https://arxiv.org/pdf/2011.10680) HAWQV3: Dyadic Neural Network Quantization [[code](https://github.com/zhen-dong/hawq)] ![GitHub Repo stars](https://img.shields.io/github/stars/zhen-dong/hawq)
- [ICML 2021](https://proceedings.mlr.press/v139/hubara21a/hubara21a.pdf) Accurate Post Training Quantization With Small Calibration Sets 
- [ICML 2021](https://icml.cc/virtual/2021/poster/9811) I-BERT: Integer-only BERT Quantization [[code](https://github.com/kssteven418/I-BERT)] ![GitHub Repo stars](https://img.shields.io/github/stars/kssteven418/I-BERT)
- [NeurIPS 2021](https://arxiv.org/abs/2106.14156) Post-Training Quantization for Vision Transformer 
- [NeurIPS 2021](https://arxiv.org/abs/2105.11010) Post-Training Sparsity-Aware Quantization [[code](https://github.com/gilshm/sparq)] ![GitHub Repo stars](https://img.shields.io/github/stars/gilshm/sparq)
- [NeurIPS 2021](https://arxiv.org/abs/2105.08952) BatchQuant: Quantized-for-all Architecture Search with Robust Quantizer 
- [NeurIPS 2021](https://arxiv.org/abs/2111.02625) Qimera: Data-free quantization with synthetic boundary supporting samples [[code](https://github.com/iamkanghyunchoi/qimera)] ![GitHub Repo stars](https://img.shields.io/github/stars/iamkanghyunchoi/qimera)


### 2020
- [AAAI 2020](https://arxiv.org/abs/1909.05840) Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT [[code](https://github.com/sIncerass/QBERT)] ![GitHub Repo stars](https://img.shields.io/github/stars/sIncerass/QBERT)
- [AAAI 2020](https://cdn.aaai.org/ojs/6134/6134-13-9359-1-10-20200513.pdf) Towards Accurate Low Bit-Width Quantization with Multiple Phase Adaptations 
- [CVPR 2020](https://arxiv.org/abs/2001.00281) ZeroQ: A Novel Zero Shot Quantization Framework [[code](https://github.com/amirgholami/ZeroQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/amirgholami/ZeroQ)
- [CVPR 2020](https://arxiv.org/abs/2004.09576) LSQ+: Improving Low-bit Quantization Through Learnable Offsets and Better Initialization 
- [CVPR 2020](https://arxiv.org/abs/1912.09666) AdaBits: Neural Network Quantization With Adaptive Bit-Widths [[code](https://github.com/deJQK/AdaBits)] ![GitHub Repo stars](https://img.shields.io/github/stars/deJQK/AdaBits)
- [CVPR 2020](https://arxiv.org/abs/1912.08883) Adaptive Loss-aware Quantization for Multi-bit Networks [[code](https://github.com/zqu1992/ALQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/zqu1992/ALQ)
- [ECCV 2020](https://arxiv.org/abs/2007.09952) HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs [[code](https://github.com/sony-si/ai-research)] ![GitHub Repo stars](https://img.shields.io/github/stars/sony-si/ai-research)
- [ECCV 2020](https://arxiv.org/abs/2003.03603) Generative Low-bitwidth Data Free Quantization [[code](https://github.com/xushoukai/GDFQ)] ![GitHub Repo stars](https://img.shields.io/github/stars/xushoukai/GDFQ)
- [ICML 2020](https://arxiv.org/pdf/2402.16121v1) Towards Accurate Post-training Quantization for Reparameterized Models [[code](https://github.com/ilur98/DLMC-QUANT)] ![GitHub Repo stars](https://img.shields.io/github/stars/ilur98/DLMC-QUANT)
- [ICML 2020](https://arxiv.org/pdf/2004.10568) Up or Down? Adaptive Rounding for Post-Training Quantization [[code](https://github.com/itayhubara/CalibTIP)] ![GitHub Repo stars](https://img.shields.io/github/stars/itayhubara/CalibTIP)
- [NeurIPS 2020](https://arxiv.org/abs/2009.08695) Searching for Low-Bit Weights in Quantized Neural Networks [[code](https://github.com/zhaohui-yang/Binary-Neural-Networks)] ![GitHub Repo stars](https://img.shields.io/github/stars/zhaohui-yang/Binary-Neural-Networks)
- [NeurIPS 2020](https://arxiv.org/abs/2002.07686) Robust Quantization: One Model to Rule Them All [[code](https://github.com/moranshkolnik/RobustQuantization?utm_source=catalyzex.com)] ![GitHub Repo stars](https://img.shields.io/github/stars/moranshkolnik/RobustQuantization?utm_source=catalyzex.com)
